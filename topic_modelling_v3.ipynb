{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=0.04, max_features=None, min_df=10,\n",
       "                ngram_range=(1, 1), preprocessor=None,\n",
       "                stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                      'afterwards', 'again', 'against', 'all',\n",
       "                                      'almost', 'alone', 'along', 'already',\n",
       "                                      'also', 'although', 'always', 'am',\n",
       "                                      'among', 'amongst', 'amoungst', 'amount',\n",
       "                                      'an', 'and', 'another', 'any', 'anyhow',\n",
       "                                      'anyone', 'anything', 'anyway',\n",
       "                                      'anywhere', ...}),\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
    "vectorizer = CountVectorizer(lowercase=True, stop_words=ENGLISH_STOP_WORDS, analyzer='word', binary=True, min_df=10, max_df=.04)\n",
    "vectorizer.fit(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 10299)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(X_train, n_topics, alpha, beta, n_iter=10):\n",
    "    n_kw = np.zeros((n_topics, X_train.shape[1]))   #счетчики\n",
    "    n_dk = np.zeros((X_train.shape[0], n_topics))\n",
    "    n_k = np.zeros(n_topics)\n",
    "    docs, words = X_train.nonzero()\n",
    "    z = np.random.choice(n_topics, len(docs))\n",
    "    for doc, word, i in zip(docs, words, z):\n",
    "        n_dk[doc, i] += 1\n",
    "        n_kw[i, word] += 1\n",
    "        n_k[i] += 1\n",
    "    for cur_iter in tqdm(range(n_iter)):\n",
    "        for i in range(len(docs)):\n",
    "            n_dk[docs[i], z[i]] -= 1\n",
    "            n_kw[z[i], words[i]] -= 1\n",
    "            n_k[z[i]] -= 1\n",
    "            p = (n_dk[docs[i], :] + alpha[:]) * (n_kw[:, words[i]] + beta[words[i]]) / (n_k[:] + beta.sum())\n",
    "            z[i] = np.random.choice(np.arange(n_topics), p=p / p.sum())\n",
    "            n_dk[docs[i], z[i]] += 1\n",
    "            n_kw[z[i], words[i]] += 1\n",
    "            n_k[z[i]] += 1\n",
    "    return z, n_kw, n_dk, n_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 50/50 [30:58<00:00, 37.18s/it]\n"
     ]
    }
   ],
   "source": [
    "n_topics = 20\n",
    "z, n_kw, n_dk, n_k = lda(X_train, n_topics, np.ones(n_topics), np.ones(X_train.shape[1]), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic №0:\tcomments\tdetails\texpect\timportant\tlater\tlonger\tnote\toriginal\tpossibly\tprovide\n",
      "Topic №1:\tbike\tbuy\tcar\tcars\tengine\tmiles\tride\troad\tspeed\tturn\n",
      "Topic №2:\tchildren\tcountry\tisrael\tisraeli\tjews\tkilled\tland\tmilitary\tpopulation\twar\n",
      "Topic №3:\t11\t14\t18\t24\t25\t45\t50\tal\thi\tmax\n",
      "Topic №4:\tanybody\tblack\tcheers\tcomes\tcouple\tespecially\tregards\treply\tsorry\twondering\n",
      "Topic №5:\tchip\tclinton\tclipper\tencryption\tfederal\tkey\tkeys\tlaw\tprivate\tpublic\n",
      "Topic №6:\tcouple\tdeleted\tdifference\tgoes\tguess\toh\tsorry\tsort\tsounds\tstuff\n",
      "Topic №7:\tassume\tcame\tnice\toh\tok\treading\tsimply\tsomebody\tsorry\tstuff\n",
      "Topic №8:\t12\tgame\tgames\tleague\tplay\tplayer\tplayers\tseason\tteam\twin\n",
      "Topic №9:\tagree\tbible\tchrist\tchristian\tchristians\tjesus\tman\treligion\tsaying\tword\n",
      "Topic №10:\tcard\tcomputer\tdisk\tmac\tmemory\tmonitor\tpc\tprice\tsale\tvideo\n",
      "Topic №11:\tcame\tdays\thappened\thome\tleft\tnight\tsaw\ttold\ttook\twent\n",
      "Topic №12:\tbanks\tcause\tdisease\teffect\tgordon\tmedical\tpitt\tsoon\tsurrender\tusually\n",
      "Topic №13:\tarea\tblack\tinstead\tlooked\trecall\trest\tsaw\tsmall\ttalking\tunless\n",
      "Topic №14:\tal\tbob\tcurrent\tdave\tinternet\toh\tones\tphone\tsmall\tstuff\n",
      "Topic №15:\tcost\tearth\tgeneral\tlarge\tlow\tnasa\tnational\tresearch\tscience\tspace\n",
      "Topic №16:\tcouldn\tdifference\texperience\tjob\tlonger\tmatter\tposition\tputting\tresponse\tseriously\n",
      "Topic №17:\tapplication\tcode\tfile\tfiles\tftp\trunning\tserver\tuser\tversion\twindow\n",
      "Topic №18:\tcompound\tfbi\tgas\thear\tkoresh\tsays\tsorry\tstarted\ttv\twouldn\n",
      "Topic №19:\t1993\taddress\tapr\tarticle\tca\tinternet\tnet\tnews\tsubject\tuniversity\n"
     ]
    }
   ],
   "source": [
    "top_words = np.argsort(n_kw, axis=1)[:, :-11:-1]\n",
    "for i in range(20):\n",
    "    doc = np.zeros((1, X_train.shape[1]))\n",
    "    for j in top_words[i]:\n",
    "        doc[0, j] = 1\n",
    "    print('Topic №{}:\\t{}'.format(i, '\\t'.join(vectorizer.inverse_transform(doc)[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили следующие топики:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Машины\n",
    "2) Криптография\n",
    "3) Спорт\n",
    "4) Религия\n",
    "5) Компьютер\n",
    "6) Наука\n",
    "7) Работа\n",
    "8) Сети\n",
    "9) Преступления\n",
    "10) Войны "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В исходном датасете присутствуют все эти темы.\n",
    "При увеличении количества итераций или расширении словаря мы получим более точные данные, но проблема в том, что это будет занимать достаточно много времени."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
